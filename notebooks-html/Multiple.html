
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have explored ways to use multiple features to predict a categorical variable, it is natural to study ways of using multiple predictor variables to predict a quantitative variable. A commonly used method to do this is called <em>multiple regression</em>.</p>
<p>We will start with an example to review some fundamental aspects of <em>simple</em> regression, that is, regression based on one predictor variable.</p>
<h3 id="Example:-Simple-Regression">Example: Simple Regression<a class="anchor-link" href="#Example:-Simple-Regression">&#182;</a></h3><p>Suppose that our goal is to use regression to estimate height based on weight, based on a sample that looks consistent with the regression model. Suppose the observed correlation $r$ is 0.5, and that the summary statistics for the two variables are as in the table below:</p>
<table>
<thead><tr>
<th style="text-align:right"></th>
<th style="text-align:center"><strong>average</strong></th>
<th style="text-align:center"><strong>SD</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">height</td>
<td style="text-align:center">69 inches</td>
<td style="text-align:center">3 inches</td>
</tr>
<tr>
<td style="text-align:right">weight</td>
<td style="text-align:center">150 pounds</td>
<td style="text-align:center">20 pounds</td>
</tr>
</tbody>
</table>
<p>To calculate the equation of the regression line, we need the slope and the intercept.</p>
$$
\mbox{slope} ~=~ \frac{r \cdot \mbox{SD of }y}{\mbox{SD of }x} ~=~
\frac{0.5 \cdot 3 \mbox{ inches}}{20 \mbox{ pounds}} ~=~ 0.075 ~\mbox{inches per pound}
$$$$
\mbox{intercept} ~=~ \mbox{average of }y - \mbox{slope}\cdot \mbox{average of } x
~=~ 69 \mbox{ inches} ~-~ 0.075 \mbox{ inches per pound} \cdot 150 \mbox{ pounds}
~=~ 57.75 \mbox{ inches}
$$<p>The equation of the regression line allows us to calculate the estimated height, in inches,
based on a given weight in pounds:</p>
$$
\mbox{estimated height} ~=~ 0.075 \cdot \mbox{given weight} ~+~ 57.75
$$<p>The slope of the line is measures the increase in the estimated height per unit increase in weight. The slope is positive, and it is important to note that this does not mean that we think people get taller if they put on weight. The slope reflects the difference in the average heights of two groups of people that are 1 pound apart in weight. Specifically, consider a group of people whose weight is $w$ pounds, and the group whose weight is $w+1$ pounds. The second group is estimated to be 0.075 inches taller, on average. This is true for all values of $w$ in the sample.</p>
<p>In general, the slope of the regression line can be interpreted as the average increase in $y$ per unit increase in $x$. Note that if the slope is negative, then for every unit increase in $x$, the average of $y$ decreases.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiple-Predictors">Multiple Predictors<a class="anchor-link" href="#Multiple-Predictors">&#182;</a></h3><p>In multiple regression, more than one predictor variable is used to estimate $y$. For example, we might want to estimate blood pressure based on both weight and age. Then the multiple regression model would involve two slopes, an intercept, and random errors as before:</p>
<p>blood pressure $~=~ \mbox{slope}_w * \mbox{weight} ~+~ \mbox{slope}_a * \mbox{age} ~+~ \mbox{intercept} ~+~ \mbox{random error}$</p>
<p>Our goal would be to find the estimated blood pressure using the best estimates of the two slopes and the intercept; as before, the "best" estimates are those that minimize the mean squared error of estimation.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The mathematics of multiple regression is complicated, and Python code for multiple regression requires knowledge of data structures that we have not covered in this course. Therefore we will just examine output to note some important parallels and differences between multiple regression and simple regression.</p>
<p>To start off, we will revisit the Hodgkin's disease data that we explored in our introduction to inference in simple regression. The table consists of data for a random sample of 22 patients. The variables are height in centimeters, a measure of radiation, a measure of chemotherapy, a baseline score on a test of the health of the lungs, and the score on the test taken 15 months after the treatment. It is apparent from the data that the scores tend to go down after 15 months; our goal will be to try to predict the drop in scores from baseline to 15 months, using combinations of the other variables.</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">hodgkins</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">&#39;hodgkins.csv&#39;</span><span class="p">)</span>
<span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;drop&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;base&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;month15&#39;</span><span class="p">]</span>  <span class="c"># drop in scores after treatment</span>
<span class="n">hodgkins</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[4]:</div>

<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>height</th> <th>rad</th> <th>chemo</th> <th>base</th> <th>month15</th> <th>drop</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>164   </td> <td>679 </td> <td>180  </td> <td>160.57</td> <td>87.77  </td> <td>72.8  </td>
        </tr>
    </tbody>
        <tr>
            <td>168   </td> <td>311 </td> <td>180  </td> <td>98.24 </td> <td>67.62  </td> <td>30.62 </td>
        </tr>
    </tbody>
        <tr>
            <td>173   </td> <td>388 </td> <td>239  </td> <td>129.04</td> <td>133.33 </td> <td>-4.29 </td>
        </tr>
    </tbody>
        <tr>
            <td>157   </td> <td>370 </td> <td>168  </td> <td>85.41 </td> <td>81.28  </td> <td>4.13  </td>
        </tr>
    </tbody>
        <tr>
            <td>160   </td> <td>468 </td> <td>151  </td> <td>67.94 </td> <td>79.26  </td> <td>-11.32</td>
        </tr>
    </tbody>
        <tr>
            <td>170   </td> <td>341 </td> <td>96   </td> <td>150.51</td> <td>80.97  </td> <td>69.54 </td>
        </tr>
    </tbody>
        <tr>
            <td>163   </td> <td>453 </td> <td>134  </td> <td>129.88</td> <td>69.24  </td> <td>60.64 </td>
        </tr>
    </tbody>
        <tr>
            <td>175   </td> <td>529 </td> <td>264  </td> <td>87.45 </td> <td>56.48  </td> <td>30.97 </td>
        </tr>
    </tbody>
        <tr>
            <td>185   </td> <td>392 </td> <td>240  </td> <td>149.84</td> <td>106.99 </td> <td>42.85 </td>
        </tr>
    </tbody>
        <tr>
            <td>178   </td> <td>479 </td> <td>216  </td> <td>92.24 </td> <td>73.43  </td> <td>18.81 </td>
        </tr>
    </tbody>
</table>
<p>... (12 rows omitted)</p</div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Correlation-Matrix">Correlation Matrix<a class="anchor-link" href="#Correlation-Matrix">&#182;</a></h3><p>A natural first step is to see which variables are correlated with the drop. Here is the correlation matrix. It is apparent that <code>chemo</code> and <code>base</code> are most highly correlated with <code>drop</code>. Of course <code>month15</code> is correlated with <code>drop</code> as well, but we cannot use it for prediction because we will not have the value of that variable for a new patient.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To start off, let us perform the simple regression of <code>drop</code> on just <code>base</code>, the baseline score. Here is the scatter diagram and regression line, produced using the function <code>scatter_fit</code> that we defined earlier in the course. You can see that the regression model fits reasonably well.</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">scatter_fit</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">,</span> <span class="s">&#39;base&#39;</span><span class="p">,</span> <span class="s">&#39;drop&#39;</span><span class="p">)</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAcgAAAEtCAYAAACMKPDbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtclHXe//HXoCIG65GjKHhCRUUsSb211DBlPWUaZm15
BDzubictvavbPKy2JurulqbO4CHNNKWM0tSk8oxWvxjME+p6DsEDKiYqML8/vJ071skogYsZ3s/H
o8fCzDXj+wMsb+aa73VdpuzsbBsiIiJSiJvRAURERMoiFaSIiIgDKkgREREHVJAiIiIOqCBFREQc
UEGKiIg4oIIUERFxwCkKMi8vj8mTJxMeHo6/vz/h4eFMnTqV/Pz8QttNnz6d0NBQAgIC6NWrFwcO
HDAosYiIODunKMj4+HgWLVrEjBkz2LNnD2+++SYWi4VZs2bZt5kzZw5z585lxowZJCcn4+PjQ9++
fcnJyTEwuYiIOCunKMjvvvuO7t27ExUVRd26de0ff/PNNwDYbDbmzZvHCy+8QO/evQkNDWXevHnk
5OSwevVqg9OLiIgzcoqC7Nq1K1u2bCE9PR2AAwcOsG3bNqKiogA4fvw4mZmZREZG2h/j4eFB+/bt
SUlJMSSziIg4t4pGByiK2NhYzpw5Q5s2bahYsSJ5eXmMHTuWYcOGAXD27FkAfHx8Cj3O29ubjIyM
Us8rIiLOzykK8t1332X58uUkJCTQtGlTrFYr48ePJygoiIEDB971sSaTqZRSioiIK3GKgoyPj2fs
2LH07dsXgNDQUE6ePMns2bMZOHAgfn5+AGRlZREYGGh/XFZWFr6+voZkFhER5+YU70HabDbc3ApH
dXNzw2a7daWu4OBg/Pz8SE5Ott+fm5vLrl27aNu2balmFRER1+AUBdmzZ0/mzJnDxo0bOX78OElJ
ScydO5devXoBt3ajjho1ijlz5pCUlMS+ffsYPXo0Xl5eREdHG5y+bLm90Km8Ka9zg2Yvj8rr3MXN
KXaxTps2jT/84Q+MHTuWrKws/Pz8GDJkCC+//LJ9m+eee45r164xbtw4srOziYiIIDExEU9PTwOT
i4iIszJlZ2fbjA4hpSc9PZ2QkBCjY5S68jo3aPbyOHt5nbu4OcUuVhERkdKmghQREXFABSkiIuKA
ClJERMQBFaSIiIgDKkgREREHVJAiIiIOqCBFREQcUEGKiIg4oIIUERFxwCnOxSoiIkVnMrnz7be3
Xv+EhRXg7m5wICelV5AiIi4mIyOArl296NrVi7Q0/Zr/vfSVExERcUC7WEVEXIy//49s2uQB3NrF
Kr+PClJExMXYbDdo3VrFeK+0i1VERMQBFaSIiIgDKkgREREH9B6kiEgJuHED+yEWOhbROekVpIhI
CUhLc9OxiE5O3zUREREHtItVRKQEhIUVsGlTjv1jcT4qSBGREuDujo5FdHLaxSoiIuKAClJERMQB
FaSIiIgDKkgREREHVJAiIiIOqCBFREQcUEGKiIg4oIIUERFxQAUpIiLigApSRETEARWkiIiIAypI
ERERB1SQIiIiDqggRUREHHCagszIyGDkyJE0atQIf39/2rVrx/bt2wttM336dEJDQwkICKBXr14c
OHDAoLQiIuLsnKIgs7OziYqKwmQy8eGHH7J7925mzJiBj4+PfZs5c+Ywd+5cZsyYQXJyMj4+PvTt
25ecnBwDk4uIiLNyigsm//Of/6R27drMmzfPfltQUJD9Y5vNxrx583jhhRfo3bs3APPmzSMkJITV
q1czZMiQ0o4sUq7duAFpabf+/g4LK8Dd3eBAIr+DU7yC/Oyzz3jggQcYOnQoISEhPPzwwyxcuNB+
//Hjx8nMzCQyMtJ+m4eHB+3btyclJcWIyCLlWlqaG127etG1q5e9KEWcjVP85B47dgyLxUKDBg1I
TExk5MiRTJo0yV6SZ8+eBSi0yxXA29ubzMzMUs8rIiLOzyl2sRYUFNC6dWtef/11AMLCwjh69Chm
s5m4uLi7PtZkMpVGRBH5mbCwAjZtyrF/LOKMnKIg/f39adKkSaHbQkJCOHXqFAB+fn4AZGVlERgY
aN8mKysLX1/fX3ze9PT0Ekhb9mnu8seI2atWvfW/x4+X+j9dSHn9vpfHuUNCQor1+ZyiINu1a8eh
Q4cK3Xb48GH7Qp3g4GD8/PxITk6mVatWAOTm5rJr1y6mTJnyi89b3F9MZ5Cenq65yxnNXv5mL69z
FzeneA9y9OjRfPPNN8THx3P06FE+/vhjFixYQGxsLHBrN+qoUaOYM2cOSUlJ7Nu3j9GjR+Pl5UV0
dLTB6UVExBk5xSvI+++/n+XLlzN58mTeeust6taty2uvvUZMTIx9m+eee45r164xbtw4srOziYiI
IDExEU9PTwOTi4iIs3KKggTo1q0b3bp1u+s248ePZ/z48aWUSEREXJlT7GIVEREpbSpIERERB1SQ
IiIiDqggRUREHFBBioiIOKCCFBFxMVduXmHud3M5e/Ws0VGcmgpSRMRFWDOt/PWLv9InuQ/fnf2O
3LxcoyM5Nac5DlJERO50Pe86H6d/jMVq4UzOGYaEDeHDzh/SrkU7o6M5PRWkiIgTOn7pOIvTFrNs
3zJaeLfguYjniKofRUW3iuXyROUlQQUpIuIkCmwFJB9Pxmw1k3ImhadCn2J9//U0qtHI6GguSQUp
IlLGXcy9yLIflpFgTeAP7n8gNjwWS3cLnpV0rumSpIIUESmj/t/Z/4c51cynRz4lqn4UC/64gAj/
CF0IvpSoIEVEypDcvFwSDyViSbWQ+VMmMS1j+Hbwt3jf5210tHJHBSkiUgYcu3SMBGsCy/ct537f
+xnXdhxd63WlglsFo6OVWypIERGD5Bfk88XxL7CkWvgm4xv+1OxPbBqwiQbVGxgdTVBBioiUuvPX
ztsX3dTwqEFseCxLei2hSsUqRkeTn1FBioiUApvNxrcZ32K2mll/dD09GvRgUY9FPOD/gNHR5Beo
IEVEStBPN39izaE1WFItZF/PJqZlDNM6TqNmlZpGR5NfoYIUESkBR7OPYrFaWLFvBQ8GPMir//Uq
Xep1wc2kU2A7CxWkiEgxyS/IZ8O/N2CxWkjNTOWZZs+Q/HQy9arVMzqa/A4qSBGRe5T1Uxbv7X2P
hLQE/D39GdZyGMt7L8ejoofR0eQeqCBFRH4Hm83G7h93Y7Fa2PDvDfRu1JtlvZbRyq+V0dGkmKgg
RUR+g6s3r7L6wGrMVjNXb15lWMth/L3z36nhUcPoaFLMVJAiIkVw+OJhzKlmVh5YSdvabXmjwxs8
EvyIFt24MBWkiMgvyCvI4/Ojn2O2mvnh3A8MbD6Qr57+iuBqwUZHk1KgghQR+Q+ZVzNZsncJi9MW
E/iHQGLDY+nTqA+VK1Y2OpqUIhWkiAi3Ft3sPLMTS6qFL45/weMhj/P+Y+8T7htudDQxiApSRMq1
nBs5rDqwCrPVzI38G8S0jCE+Mp7qHtWNjiYGU0GKSLl08MJBzKlmPjzwIR3qdGBax2l0rNtRi27E
TgUpIuXGzfybrDu6DnOqmYMXDjKoxSC2PbuNOn+oY3Q0KYNUkCLi8jKuZrAkbQlL9i4huGowseGx
9G7UG/cK7kZHkzJMBSkiLslms/Ht+W+ZdmgaySeS6de4H6v6rKKFTwujo4mTUEGKiEu5fP0yqw6s
wmK1cC33GqMeHMWcR+dQrXI1o6OJk1FBSpHcuAFpabcWL4SFFeCuPVNSxuw7t48EawKrD66mY92O
/L3z3/G/5k/jxo2NjiZOSgUpRZKW5kbXrl4AbNqUQ+vWBQYnupNKvPy5mX+TT498ysLUhRzNPsqg
FoPYMXAHtb1qA5Cenl7qmfRz6DpUkOIynKHEpXicyTnD4rTFLN27lIbVGxIXHkevhr2oVKGS0dH0
c+hCVJBSJGFhBWzalGP/WKS02Ww2tpzagiXVwpaTW3iiyRMk9k2kmXczo6OJi3LKI2JnzZpFjRo1
GDduXKHbp0+fTmhoKAEBAfTq1YsDBw4YlND1uLtD69YFtG5ddncZ3S7xTZtyVOIu5NL1S8z/fj5t
l7Zl/Ffj6Vi3I2kxacRHxpfJctTPoetwuleQe/bsYcmSJTRv3hyTyWS/fc6cOcydO5e5c+fSqFEj
ZsyYQd++fdmzZw9eXl4GJpbScrvExTXszdqLxWoh8VAikUGRzO4ym/aB7Qv9/74s0s+h63CqV5CX
Ll1i+PDhvPPOO1Sv/n/nSbTZbMybN48XXniB3r17Exoayrx588jJyWH16tUGJhaR3+JG/g1WH1hN
91Xd6f9xf/w9/UkZlMKinovoUKdDmS9HcS1O9Qry+eef5/HHH+ehhx7CZrPZbz9+/DiZmZlERkba
b/Pw8KB9+/akpKQwZMgQA9KKSFGdvHySJXuXsHTvUprWasrI+0fSo0GPMrHoRsovpynIJUuWcOzY
McxmM0ChvyTPnj0LgI+PT6HHeHt7k5GRUXohRaTICmwFfH3ia8xWM9tPbefJ0CdJik6iSc0mRkcT
AZykINPT05kyZQqff/45FSpUAG7tVv35q8hfcrddMkYcI1UWaO7ypyzNfuXmFZJOJbHm+Brc3dzp
H9yflzu/zH0V74PzkH6+eLOWpdlLU3mcOyQkpFifzykKcvfu3Zw/f5527drZb8vPz2fnzp0sXryY
nTt3ApCVlUVgYKB9m6ysLHx9fX/xeYv7i+kM0tPTNXc5Y8Tsjg6Wt2ZaMVvNrE1fS9d6XZnfYz7t
arcr0fcVy+v3vbzOXdycoiB79epF69at7Z/bbDbGjBlDo0aNePHFF2nYsCF+fn4kJyfTqlUrAHJz
c9m1axdTpkwxKrZIuXX7YHlbheu8svQ9ki+bOZNzhiFhQ9gzaA++nr/8h6tIWeEUBVmtWjWqVSt8
ouEqVapQrVo1mjZtCsCoUaOIj48nJCSEhg0bMnPmTLy8vIiOjjYiski5djb3BAWR70GrRWw625wX
H36OqPpRVHRzil85IoCTFKQjJpOp0K6Z5557jmvXrjFu3Diys7OJiIggMTERT09PA1OKlB8FtgK+
PP4lC60LSTmdwhNPPk3vgPX0aNuwzJ5cQuRunLYgP/300ztuGz9+POPHjzcgjUj5dTH3Isv3LSfB
moBnJU/iwuOwdLfgWUl/nIpzc9qCFBFjfX/2e8xWM0mHk4iqH8X8qPlE+EfoYH5xGSpIESmy3Lxc
Pjr0EeZUM5k/ZTKs5TC+Hfwt3vd5Gx1NpNj9poLMzs7m7bffZsOGDZw4cQKTyURQUBDdunXjz3/+
c6HTv4mI6zh26RiLrItYvm85rXxbMbbtWLrV60YFtwpGRxMpMUUuyKNHj9K7d2/OnDlDaGgoDz/8
MABHjhwhPj6e999/n6SkJBo2bFhiYUWk9BTYCvji2BeYU818k/ENf2r2JzYO2EiD6g2MjiZSKopc
kOPGjePKlSusXbuWjh07Frrv66+/5tlnn+Xll19mzZo1xR5SRErPhWsXWPbDMixWCzU8ahAbHsuS
XkuoUrGK0dFESlWRC3Lnzp2MGTPmjnIE6NSpEyNHjuTtt98u1nAiUnq+zfgWc6qZdUfX0aNBDxJ6
JNDav/WvP1DERRW5IKtWrUqNGjV+8f7q1avfcTC/iJRt1/KusebgGixWCxeuXSCmZQxTO06lVpVa
RkcTMVyRC3LQoEEsW7aMZ599lqpVqxa679KlSyxbtoxBgwYVe0CR8szROU2Lw9HsoyRYE1ixfwUR
/hFMaDeBR+s9ipvJqS4RK1KiilyQISEhmEwmHnzwQZ566in7YpzDhw/zwQcf4OvrS+PGjfnoo48K
Pa5v377Fm1ikHLl9TlOATZty7ulK9fkF+Ww8thFzqpnUzFSeafYMm5/aTL1q9YoprYhrKXJBDh8+
3P7xP//5zzvuz8rKIi4urtBtJpNJBSlisIvXLzJ7z2wSrAn43udLbHgsy3svx6Oih9HRRMq0Ihfk
J598UpI5RMSBsLACNm3KsX9cVDabjT0/7sFsNbPu8Dr6NO7D0l5Lud/v/pKKKuJyilyQt497FJHS
4+7Ob9qtevXmVdYcXMPC1IXk3MghJjyG4XWHE9E8ogRTirim33yquby8PKxWKydOnAAgKCiIVq1a
4eamN/dFjHL44mEsVgsf7P+AtrXbMrHDRCKDI3EzuZXLK8uLFIffVJBr1qzh1Vdf5ezZs4Vu9/f3
529/+xv9+vUr1nAi8svyCvL4/OjnWKwW9p7by8DmA/nq6a8IrhZsdDQRl1Dkgvzss8+Ii4ujcePG
vPTSSzRu3BiAQ4cOkZCQQFxcHJUrV6Znz54lFlbKppI6FEEcy7yaydIflrI4bTG1vWoT0zKGx0Me
p3LFykZHE3EpRS7I+Ph4wsPDWb9+PR4e/7f6rVOnTgwcOJDu3bsTHx+vgiyHivNQhHvhykVts9nY
dWYXFquFTcc20SekD8t7LyfcN9zoaCIuq8gFuX//fiZOnFioHG/z8PDgySefZNKkScUaTuS3KCtF
XZxybuTw4YEPMVvNXM+/TkzLGGY+MpPqHrpyjkhJK3JBenh4cO7cuV+8//z581SpopMZl0e/91AE
+WUHLxzEYrWwav8q2ge2Z+rDU+kU1ElnuhEpRUUuyM6dO7NgwQIeeeQROnToUOi+nTt3smDBArp0
6VLsAaXs+62HIpQUZy/qm/k3WXd0HRarhQPnDzCoxSC2PbuNOn+oY3Q0kXKpyAX5xhtvsHPnTnr1
6kWrVq0ICQkBbi3SSU1Nxd/fnzfeeKOkcor8qrJS1L9VxtUMlqQtYcneJQRXDSamZQyPhTyGewUX
ehNVxAkVuSCDg4PZunUrs2fPZuPGjaxduxaTyUTdunUZM2YMzz//PN7e3iWZVcRl2Gw2tp/ejiXV
QvKJZPo17sfKPisJ8wkzOpqI/K8iFWRubi6JiYk0adKEadOmMW3atJLOJeKSLl+/zKoDq7BYLeQX
5BMTHsOcR+dQrXLxXCquOFfyuvKqYJGiKNI7/pUrV+b5558nLS2tpPOIuKT95/czNnksLRNasuXk
Ft7s/CYpg1IY0WpEsZUj/N9K3q5dvezlVhaeS8QZFekVpMlkolGjRnecQUdEftnN/Jt8euRTzKlm
jmQfYVCLQWx/djuBfwg0OpqIFEGR34McO3Ys48aNo2fPnrRo0aIkM4k4tTM5Z1ictpile5fSoHoD
4sLj6NWwF5UqVCrxf7s4V/I6+6pgkXtV5ILctm0bPj4+dOrUiTZt2lC/fn2Hxz3Gx8cXa0ARZ2Cz
2dhyaguWVAtbTm4hukk0iX0TaebdrFRzFOdKXmddFSxSXIpckIsWLbJ/vGvXLnbt2uVwOxWklCeX
rl/ig/0fYEm14GZyIzY8lre7vk3VylWNjiYi96jIBXnx4sWSzCHiVPZm7cVitZB4KJHIoEhmdZlF
h8AOmEwmo6OJSDH5zdeDFCmvbuTf4JP0T7BYLRy7dIwhYUPYNXAXAV4BRkcTkRLwiwVZo0YNTCYT
Nput0O23/0K+ffvPPzeZTFy4cKGksooY4uTlkyzZu4Sle5fSpGYTRt4/kh4NepTKohsRMc4vFuTL
L798x22ffvopBw8epEuXLjRs2BCAI0eOsHnzZpo2bUqvXr1KLqlIKSqwFfD1ia8xW81sP7WdJ0Of
JCk6iSY1mxgdTdBJDKR0/GJBTpgwodDnixcv5sKFC6SkpNjL8bbDhw/Tu3dvAgK0q0mcW3ZuNu/v
e5+EtATcK7gT1zKO+VHz8XL3Mjqa/IwrXtpMyp4ivwf5j3/8g9jY2DvKEaBRo0bExsbyj3/8g8GD
BxdrQJHS8O3pVOK3JrDl3Fq61uvCvx79F+1qt9OiG5FyrMgF+eOPP1Kx4i9vXqFCBU6fPl0soURK
w/W863yc/jEWq4V/nz/NuQ0j4Lt9/PljL1oH6hVJWaaTGEhpKHJBhoaGYrFYiI6OJjCw8KmyTp06
hcVioVmz0j0oWuT3OHH5BIusi1i2bxnNvZvz19Z/xTe7B398rfr/bpFjaD75dTqJgZSGIhfktGnT
6NevHxEREXTv3p0GDRoAtxbpfP755wDMnz+/ZFKK3KMCWwGbj21moXUhKWdSeCr0KdZFryOk5q3r
mt64gV6RiEghRS7I//qv/+KLL75g2rRprF+/ntzcXACqVKlCly5dmDBhAs2bNy+xoCK/x8Xciyzf
t5z538yn2n3ViAuPw9Ldgmclz0Lb6RWJiPyn33SigObNm7N8+XLy8/M5d+4cAN7e3lSoUKFEwt02
a9YskpKSOHLkCO7u7kRERDBx4kRCQ0MLbTd9+nSWLl1KdnY2rVu3ZubMmTRt2rREs4nxHC35//7s
95itZpIOJxFVP4o3Wr1Bvwf7adGNiBTZ77rIW4UKFfDz88PPz6/EyxFg+/btxMXFsXHjRj755BMq
VqzI448/TnZ2tn2bOXPmMHfuXGbMmEFycjI+Pj707duXnBy9n+Tqbi/5f/SPFYn/4gO6rOjCs58+
S4NqDfhm8Dcs+OMCWtZoqXIUkd/EKU41t2bNmkKfz58/n6CgIFJSUoiKisJmszFv3jxeeOEFevfu
DcC8efMICQlh9erVDBkyxIDUUlrOXPs3BY++B+GLSc4K56WHXiKqfhQV3Er+jzcRcV1OUZD/6cqV
KxQUFFC9+q1Vh8ePHyczM5PIyEj7Nh4eHrRv356UlBQVpAvKL8hn8/HNmFPNfJPxDdH9/0TvgI10
b9NAZ1URkWLhlAU5fvx4WrZsSZs2bQA4e/YsAD4+PoW28/b2JiMjo9TzScm5cO0Cy35YhsVqoYZH
DWLDY1ncczH3VbrP6Ggi4mKcriD/+7//m927d7N+/foivad0t23S09OLM5rTcMa5f8j+gQ+Pf8iW
jC109OvIpLBJNKvWDJPJxOljRTtBhTPOXVw0e/lTHucOCQkp1udzqoKcMGECH3/8MUlJSQQHB9tv
9/PzAyArK6vQSQyysrLw9fX9xecr7i+mM0hPTy9Tc9/tpNPX8q6x5uAaLFYLF65dIKZlDP/s+U9q
Van1m/+dsjZ3adLs5W/28jp3cXOagnzllVdYu3YtSUlJNGrUqNB9wcHB+Pn5kZycTKtWrQDIzc1l
165dTJkyxYi4UkSOTjp9NPsoCdYEVuxfQYR/BBPaTaBLcBctuhGRUuUUBTl27FhWrVrFsmXLqFq1
qv09Ry8vLzw9PTGZTIwaNYr4+HhCQkJo2LAhM2fOxMvLi+joaIPTS1HYTPnsOL+e6R8t5PvM73mm
2TNsfmoz9arVMzqaiJRTTlGQFosFk8lEnz59Ct0+fvx4XnnlFQCee+45rl27xrhx48jOziYiIoLE
xEQ8PT0dPaWUEQGNMhmWMJtPf0zg4/M+xIbHsKz3MjwqehgdTUTKOacoyIsXLxZpu/HjxzN+/PgS
TiP3ymaz8U3GNyxMXciGf2+gV8NerOi3mPv97jc6moiInVMUpLiGn27+xOqDqzGnmrly4wox4TH8
vfPfqeFRw+hoIiJ3UEFKiTt88TAWq4UP9n9A29ptmdhhIo8EP4Kb6Xed6VBEpFSoIKVE5BXkseHf
GzCnmtl7bi/PNnuWr57+iuBqwb/+YCmSux0iIyL3TgUpxSrzaibv/fAei9IWUdurNjEtY3g85HEq
V6xsdDSX4+gQGREpPipIuWc2m42UH1Mwp5rZdGwTfUL6sKz3Mlr5tjI6mojI76aClN8t50aOfdHN
tbxrDGs5jJmPzKS6R3Wjo5ULYWEFbNqUY/9YRIqXClJ+s0MXDmGxWlh1YBXtA9sz5eEpdArqpEU3
pczdHe1WFSlBKshy6rcu8MgryGPd0XVYUi3sP7+fQS0GseVPW6hbtW4ppBURKX0qyHKqqAs8Mq5m
sCRtCUv2LiGoahAxLWN4rNFjWnQjIi5PBSl3sNls7Di9A3OqmeQTyfRr3I+VfVYS5hNmdDQRkVKj
giynHC3wuHLjCqv2r8JitZBXkEdMeAxzHp1DtcrVjIwqImIIFWQ59fMFHvvP78eSamH1wdV0rNuR
6Z2n07FOxyJdkFpExFWpIMupm/k3+ezIZyxMXciR7CMMajGIHQN3UNurttHRRETKBBVkOZOZm8nq
natZuncpDao3IDY8ll4Ne1GpQiWjo4mIlCkqyHLAZrOx9dRWLFYLXx77kidDnySxXyKhtUKNjiYi
UmapIF3YpeuXWLl/JRarBTeTGzEtY3ihwQu0CtUp4EREfo0K0gX9cO4HzKlmEg8lEhkUyazIWbQP
bI/JZCI9Pd3oeCIiTkEF6SJu5N8g6XAS5lQzxy8fZ3CLwaQMSsHf09/oaCIiTkkF6eROXTnF4rTF
vLf3PZrUasLI+0fSo0EPLboREblHKkgnZLPZ+Prk15hTzWw/vZ3+TfvzSfQnNKnZxOhoIiIuQwXp
RLJzs1mxfwUWqwX3Cu7EtYzj3ah38XL3MjqaiIjLUUE6AWumFYvVwsfpH9O1Xlf+9ei/aFe7nc50
IyJSglSQZdT1vOusPbwWS6qF0zmnGRI2hN2DduPn6Wd0NBGRckEFWcacuHzi1qKbH96jhXcL/tL6
L/yxwR+p6KZvlYhIadJv3TKgwFbAl8e/xGw1s+vMLp4KfYp10esIqRlidDQRkXJLBWmgi7kXWb5v
OQnWBDwreRIXHoe5uxnPSp5GRxMRKfdUkAb4/uz3mK1mkg4nEVU/ine7vcuDAQ9q0Y2ISBmigiwl
uXm5fHToIyxWC2evnmVYy2F8M/gbfO7zMTqaiIg4oIIsYccuHWORdRHL9y0n3Decl9q8RLd63ajg
VsHoaCIichcqyBJQYCvgi2NfYLFa2PPjHp5u9jQbB2ykQfUGRkcTEZEiUkGWAJvNxqK0RfRq1ItF
PRZxX6X7jI5ULG7cgLQ0NwDCwgpwdzc4kIhICVJBloAKbhVY8dgKo2MUu7Q0N7p2vXVau02bcmjd
usDgRCIiJcfN6AAiIiJlkV5BSpGFhRWwaVOO/WMREVemgpQic3dHu1VFpNxQQUqZp8VBImIEl3sP
0mw207JlS/z9/encuTM7d+40OpLco9uLg7p29bIXpYhISXOp3zaJiYlMmDCBsWPHsnXrVtq0aUP/
/v05derEV4sHAAARLUlEQVSU0dFERMTJuFRBvvPOOzzzzDMMGjSIkJAQZsyYgZ+fHwkJCUZHk3tw
e3HQpk05WhwkIqXGZQryxo0bpKam8sgjjxS6PTIykpSUFINSSXG4vTiodWu9/ygipcdlCvL8+fPk
5+fj6+tb6HZvb28yMzMNSiUiIs7KZQpSRESkOLnMYR61atWiQoUKd7xazMrKws/Pz+Fj0tPTSyNa
maO5yx/NXv6Ux7lDQkKK9flcpiDd3d1p1aoVX375JX369LHf/uWXX/L44487fExxfzGdQXp6uuYu
ZzR7+Zu9vM5d3FymIAHGjBnDiBEjeOCBB2jbti0JCQlkZmYydOhQo6OJiIiTcamC7Nu3LxcuXGDm
zJmcPXuWZs2asWrVKurUqWN0NBERcTIuVZAAMTExxMTEGB1DREScnFaxioiIOKCCFBERcUAFKSIi
4oDLvQcpIsbQZcnE1egVpIgUC12WTFyNfopFREQc0C5WESkWty9LdvtjEWenghSRYnH7smQirkK7
WEVERBxQQYqIiDigghQREXFABSkiIuKAClJERMQBFaSIiIgDKkgREREHVJAiIiIOqCBFREQcUEGK
iIg4oIIUERFxQAUpIiLigE5WXsx00VgREdegV5DFTBeNFRFxDfoNLiIi4oB2sRYzXTRWRMQ1qCCL
mS4aKyLiGrSLVURExAEVpIiIiAMqSBEREQdUkCIiIg6oIEVERBxQQYqIiDigghQREXFABSkiIuKA
ClJERMQBFaSIiIgDKkgREREHVJAiIiIOlPmCzM7OZty4cbRp04aAgABatGjBSy+9xMWLF+/Ybvjw
4QQFBREUFMSIESO4dOmSQalFRMTZlfmC/PHHH8nIyGDy5Mns3LmT+fPns2PHDmJiYgptFxsby969
e0lMTGTNmjVYrVZGjBhhUGoREXF2Zf5yV6Ghobz33nv2z+vVq8fkyZMZMGAAOTk5eHl5cfDgQTZv
3syGDRuIiIgAYPbs2XTv3p3Dhw/TqFEjo+KLiIiTKvOvIB25fPkylStX5r777gNg9+7deHl50aZN
G/s2bdu2xdPTk927dxsVU0REnJjTFWR2djZ/+9vfGDx4MG5ut+JnZmZSq1atQtuZTCa8vb3JzMw0
IqaIiDg5wwpy6tSp1KhR467/bd++vdBjcnJyePrppwkMDGTy5MkGJXduISEhRkcwRHmdGzR7eVRe
5y5uhr0HOXr0aJ566qm7bhMYGGj/OCcnh/79+2MymVi5ciXu7u72+3x9fTl//nyhx9psNs6dO4ev
r2/xBhcRkXLBsIKsWbMmNWvWLNK2V65csZfjhx9+aH/v8bY2bdqQk5PD7t277e9D7t69m6tXr9K2
bdtizy4iIq7PlJ2dbTM6xN1cuXKFfv36kZOTw/Lly/H09LTfV7NmTSpVqgRA//79OX36NP/4xz+w
2Ww8//zzBAcHs2LFCqOii4iIEyvzBbl161Yee+wxTCYTNtv/RTWZTCQlJdGhQwfg1uKdl19+mc8/
/xyA7t2789Zbb1G1alVDcouIiHMr8wUpIiJiBKc7zKOoMjIyGDlyJI0aNcLf35927drdsSp2+vTp
hIaGEhAQQK9evThw4IBBaYtPXl4ekydPJjw8HH9/f8LDw5k6dSr5+fmFtnP22bdv385TTz1Fs2bN
qFGjBu+///4d2/zajNevX2fcuHE0bNiQwMBAnn76ac6cOVNaI/xud5s9Ly+PiRMn0qFDBwIDA2na
tClxcXGcOnWq0HO44uz/6fnnn6dGjRr861//KnS7M85elLkPHz7Ms88+S3BwMLVr16ZTp04cOnTI
fr8zzg2/Pvvly5d56aWXaN68OQEBATz44IPMnTu30Da/d3aXLMjs7GyioqLsi3p2797NjBkz8PHx
sW8zZ84c5s6dy4wZM0hOTsbHx4e+ffuSk5NjYPJ7Fx8fz6JFi5gxYwZ79uzhzTffxGKxMGvWLPs2
rjD7Tz/9RIsWLZg+fTpVqlTBZDIVur8oM06YMIFPP/2UhIQE1q1bx5UrVxgwYAAFBQWlPc5vcrfZ
r169itVqZdy4cWzZsoX333+fU6dOER0dXeiPJFec/efWrl3Ld999R0BAwB3bOOPsvzb3sWPHiIqK
on79+iQlJbFz505ef/31Qms2nHFu+PXZJ0yYwObNm5k/fz67d+/mpZdeYtKkSaxcubLQNr9ndpfc
xXr7vK3r1693eL/NZqNp06aMGDGCF198EYDc3FxCQkKYMmUKQ4YMKcW0xWvAgAHUqlWr0F9QI0eO
5OLFi6xcudIlZ69Tpw5vvfUWTz/9NFC07++lS5cICQlh7ty5REdHA3D69GnCwsJYvXo1kZGRhs3z
W/zn7I4cPHiQdu3asWPHDkJDQ11+9hMnTvDHP/6RtWvX8sQTTzB8+HD+/Oc/A7jE7I7mjo2Nxc3N
jQULFjh8jCvMDY5nb9++PY899hjjx4+339azZ0+aN2/OjBkz7ml2l3wF+dlnn/HAAw8wdOhQQkJC
ePjhh1m4cKH9/uPHj5OZmVnoC+Ph4UH79u1JSUkxInKx6dq1K1u2bCE9PR2AAwcOsG3bNqKiogDX
nv22osz4/fffc/PmzULbBAYG0qRJE5f5Otx2+fJlAKpXrw649ux5eXnExsYybtw4hwfLu+LsBQUF
bNiwgSZNmvDEE0/QqFEjIiMj+eijj+zbuOLctz366KOsX7+e06dPA5CSkkJaWhqPPvoocG+zl/mT
lf8ex44dw2KxMGbMGF588UWsViuvvPIKAHFxcZw9exag0C5XAG9vbzIyMko9b3GKjY3lzJkztGnT
hooVK5KXl8fYsWMZNmwYgEvPfltRZszMzKRChQp3HIvr4+NDVlZW6QQtBTdu3OC1116je/fuBAQE
AK49+/Tp0/H29mbo0KEO73fF2bOyssjJyWHWrFm8+uqrTJo0ia+//pq4uDg8PT3p1q2bS85926RJ
kxgxYgQtWrSgYsVblfbWW2/RrVs34N6+5y5ZkAUFBbRu3ZrXX38dgLCwMI4ePYrZbCYuLu6uj/2l
9zScxbvvvsvy5ctJSEigadOmWK1Wxo8fT1BQEAMHDrzrY5199qIoDzPelpeXx/Dhw7ly5Uqh92Nc
1datW1mxYgVbt24tdPvPDw9zRbffR+vZsyejR48GoEWLFnz//fcsXLjQXhSu6rXXXuPbb7/lgw8+
oG7dumzfvp3XXnuNunXr0qVLl3t6bpfcxerv70+TJk0K3RYSEmJfyefn5wdwx18PWVlZTn9quvj4
eF588UX69u1LaGgoAwYMYMyYMcyePRtw7dlvK8qMvr6+5Ofnc+HChULbZGZmusTXIS8vj5iYGPbv
38/atWvtu1fBdWffvn07GRkZNGnSBG9vb7y9vTl58iRvvPEGLVq0AFxz9lq1alGxYsW7/s5zxbnh
1qK0d999l6lTpxIVFUWzZs2Ii4ujX79+9tXL9zK7SxZku3btCi1vhltLoIOCggAIDg7Gz8+P5ORk
+/25ubns2rXL6U9NZ7PZ7Fc5uc3Nzc3+V7Qrz35bUWZs1aoVlSpVKrTN6dOnOXTokNN/HW7evMnQ
oUPZv38/SUlJd+xqdtXZY2Nj2bFjB9u2bWPbtm1s3bqVgIAAxowZw9q1awHXnN3d3Z0HHnjgrr/z
XHFuuPX77td+593L7C65i3X06NF069aN+Ph4+vbti9VqZcGCBUycOBG4tZtt1KhRxMfHExISQsOG
DZk5cyZeXl72VU7OqmfPnsyZM4fg4GCaNGmC1Wpl7ty59lVfrjL71atXOXLkCHBrF9PJkyexWq3U
rFmTOnXq/OqM1apVY+DAgUycOBEfHx+qV6/Oq6++SosWLejcubOBk/26u80eEBDA4MGD+f7771mx
YgU2m83+nmy1atXw8PBw2dnr1KmDt7d3oe0rVqyIr68vDRs2BJz3+/5rc//1r39l6NChtG/fnocf
fpitW7fy0Ucf2Y8ZdNa54ddn79SpE5MmTcLT05M6deqwfft2Vq5cab/i073M7pKHeQBs3LiRyZMn
c/jwYerWrUtcXBzDhw8vtM2bb77J4sWLyc7OJiIigpkzZ9K0aVODEhePq1evMn36dD755BOysrLw
8/MjOjqal19+udAVUJx99tunIAQKnYbwT3/6E++88w7w6zPeXsCyevVqcnNz6dSpE/Hx8dSuXbv0
B/oN7jb7K6+8Qnh4+B2nZgQK/aHkirPf/r7/XMuWLQsd5gHOOXtR5n7//feZNWsWp0+fpmHDhrz4
4ov069fP/hzOODf8+uznzp1j0qRJJCcnc+HCBft6i+L4nrtsQYqIiNwLl3wPUkRE5F6pIEVERBxQ
QYqIiDigghQREXFABSkiIuKAClJERMQBFaSIiIgDKkiRMmj69OnUqFHD6a+0IOLMVJAiIiIOqCBF
REQcUEGKiIg4oIIUKcMuXLhAbGwsQUFB1KtXjxdeeIGrV6/a71+3bh0DBgygefPm+Pn5ERYWxv/8
z/9w/fr1Qs+TmZnJX/7yF/t2jRs3pn///hw4cKDQdsnJyfTo0YM6depQp04doqOjSUtLK5VZRcoa
l7zclYirGDZsGIGBgUycOBGr1crixYs5ffo0q1atAm5dwaFKlSqMHDmSqlWrsnv3bubOncvp06ex
WCz25xk8eDD79u1j+PDhBAcHc+7cOXbs2MGRI0fsVzj58MMPGTFiBJGRkUycOJHc3FyWLFlCjx49
SE5OJiQkxJCvgYhRdDUPkTJo+vTpzJgxg65du9rLEGDatGm89dZbfPzxx3Tq1Ilr165RpUqVQo+d
NWsWU6dOJS0tjcDAQLKzs6lfvz5TpkwpdAmgn7t69SrNmzend+/e9iuxA2RnZ/Pggw/SuXNnFi5c
WDLDipRR2sUqUobFxcUV+nzkyJEAbNiwAcBejgUFBVy6dInz58/Ttm1bbDYbVqvVvo27uztbt24l
Ozvb4b/z5ZdfcunSJaKjozl//rz9v/z8fNq1a8fWrVtLakSRMku7WEXKsIYNGxb6vGbNmlSvXp0T
J04AsG/fPiZOnMj27du5du1aoW0vX74MQOXKlXnjjTd4/fXXCQkJISIigq5duzJgwAACAwMB7Fds
f/zxxx3mqFChQrHOJeIMVJAiTub2FdUvX75M79698fLy4vXXX6dBgwZ4eHhw5swZRo8eTUFBgf0x
o0aNokePHqxbt46vvvqKt956i1mzZvHBBx/w0EMP2bedN28eAQEBhswlUtaoIEXKsMOHD9OgQQP7
5+fPn+fSpUsEBQWxZcsWLly4wHvvvUf79u3t23z55ZcOnys4OJhRo0YxatQozpw5w8MPP0x8fDwP
PfQQ9evXB6BWrVp06tSpZIcScRJ6D1KkDPvPhTHvvvsuAFFRUfbdnj9/pVhQUMA777xT6DHXrl27
Y/dr7dq18fb2tu+GjYyMpFq1asyaNYubN2/ekeP8+fP3PoyIk9ErSJEy7Mcff6R///5069aNvXv3
snTpUrp06UKnTp3Izs6mZs2ajBo1iuHDh1OxYkU++eSTQsdJAqSnp/PYY4/Rt29fmjRpQuXKldm4
cSOHDh1i6tSpAFStWpXZs2cTFxdHx44deeKJJ/Dx8eHkyZNs3ryZ0NBQ5s6da8SXQMQwKkiRMshk
MmEymbBYLMycOZMpU6bg5ubG4MGD7aVWvXp1Vq1axWuvvcabb76Jl5cXjz32GEOHDqVDhw7256pb
ty5PPvkkW7ZsYfXq1ZhMJho1asTbb7/NM888Y9+ub9+++Pv7M2vWLN5++22uX79OQEAAbdu2Zdiw
YaX+NRAxmo6DFBERcUDvQYqIiDigghQREXFABSkiIuKAClJERMQBFaSIiIgDKkgREREHVJAiIiIO
qCBFREQcUEGKiIg4oIIUERFx4P8D7yvj74LP9hwAAAAASUVORK5CYII=
"
></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The regression output that we have seen thus far has been rather straightforward, based on code that we wrote as a collection of straightfoward functions. The output display below is produced by Python and is very similar to regression output produced by many statistical systems: they contain a large number of summary statistics, many of which are not needed for getting a solid initial grasp of the results.</p>
<p>We will ignore the third display table altogether, and focus on the first two.</p>
<p>In the top table, much of the left hand side is self-explanatory. "OLS" stands for "ordinary least squares," which is a name for our regression model. You can ignore "Df Residuals" for now, and also "Covariance Type," but note the "Df Model" simply gives the number of predictor variables. It is 1 because we have just one predictor, <code>base</code>.</p>
<p>We will ignore all of the right hand side of the table other than "R-squared". According to the correlation matrix above, the correlation between <code>drop</code> and <code>base</code> is just over 0.63; square than number to get 0.397. We will come back to this quantity, to see how its definition can be extended in the case of multiple regression.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The "coef" column of the second table provides the intercept and slope of the regression line:</p>
$$
\mbox{estimate of drop} ~=~ 0.5447 \cdot \mbox{base} ~-~ 32.1721
$$<p>The results for the slope and intercept are the same as what we would get based on our familiar methods for calculating those quantities:</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">slope</span> <span class="o">=</span> <span class="mf">0.630183</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;drop&#39;</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;base&#39;</span><span class="p">])</span>
<span class="n">slope</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[36]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>0.54472722699276954</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;drop&#39;</span><span class="p">])</span> <span class="o">-</span> <span class="n">slope</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;base&#39;</span><span class="p">])</span>
<span class="n">intercept</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[37]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>-32.172182995494026</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice the interval (0.232, 0.858), which is labeled to be a 95% confidence interval for the slope. Our method for computing such an interval would be to bootstrap the scatter diagram many times, compute the slope of the regression line, each time, draw the empirical histogram of the slopes, and pick off the central 95% interval. The interval in the display above has been computed using mathematical formulae for the endpoints; our bootstrap intervals are likely to be very close.</p>
<p>Note also that the interval does not contain 0. We can therefore conclude, at least with 95% confidence, that the slope of the true line is not 0; in other words, there is a genuine linear trend in the relation between <code>drop</code> and <code>base</code>.</p>
<p>The other columns of the second table address the question of whether the true slope is 0. We will return to those later, but in practical terms they are essentially redundant because of the information in the confidence intervals.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Definition of $R^2$, consistent with our old $r^2$</strong></p>
<p>When we studied simple regression, we had noted that</p>
$$
r ~=~ \frac{\mbox{SD of fitted values of }y}{\mbox{SD of observed values of } y}
$$<p>Let us use our old functions to compute the fitted values and confirm that this is true for our example:</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">fitted</span> <span class="o">=</span> <span class="n">fitted_values</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">,</span> <span class="s">&#39;base&#39;</span><span class="p">,</span> <span class="s">&#39;drop&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">fitted</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">hodgkins</span><span class="p">[</span><span class="s">&#39;drop&#39;</span><span class="p">])</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[38]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>0.63018263544448383</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because variance is the square of the standard deviation, we can say that</p>
$$
0.397 ~=~r^2 ~=~ \frac{\mbox{variance of fitted values of }y}{\mbox{variance of observed values of }y}
$$</div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that this way of thinking about $r^2$ involves only the estimated values and the observed values, <em>not the number of predictor variables</em>. Therefore, it motivates the definition of <em>multiple $R^2$</em>:</p>
$$
R^2 ~=~ \frac{\mbox{variance of fitted values of }y}{\mbox{variance of observed values of }y}
$$<p>It is a fact of mathematics that this quantity is always between 0 and 1. With multiple predictor variables, there is no clear interpretation of a sign attached to the square root of $R^2$. Some of the predictors might be positively associated with $y$, others negatively. An overall measure of the fit is provided by $R^2$. In the examples below, we will address the question of whether or not it is a good idea to try to get the biggest possible value of $R^2$.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiple-regression-of-the-drop-in-scores,-using-baseline-and-chemo-as-predictors">Multiple regression of the drop in scores, using baseline and chemo as predictors<a class="anchor-link" href="#Multiple-regression-of-the-drop-in-scores,-using-baseline-and-chemo-as-predictors">&#182;</a></h3><p>We are now well placed to peform and interpret the multiple regression of <code>drop</code> based on <code>chemo</code> and <code>base</code> as the predictors. Here is a graph that demonstrates the results. The scatter plot is now three dimensional, with the response variable <code>drop</code> plotted on the vertical axis. The fitted values all lie on the green plane. Some of the points are above the plane, some below. The mean squared distance between the points and this plane is the smallest among all planes.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The numerical results are displayed below. Notice the "Df Model", the number of predictors, has risen from 1 to 2. The regression equation, to be used for estimation and prediction, is</p>
$$
\mbox{estimated drop} ~=~ 0.5655 \cdot \mbox{base} ~-~ 0.1898 \cdot \mbox{chemo} ~+~ 0.992
$$<p>There is now a 95% confidence interval for each of the two true slopes; neither of the intervals contains 0.</p>
<p>To understand what the slope means when there are multiple predictors, keep in mind that the slope corresponding to a predictor is the estimated increase in $y$ per unit change in that predictor, <em>provided all the other predictors are held constant</em>. To see whether it is actually reasonable to consider increasing <code>base</code> by while holding <code>chemo</code> constant, it is necessary to examine the relation between <code>base</code> and <code>chemo</code>: if they are highly correlated, you can't change one without also changing the other. In fact, the correlation between <code>base</code> and <code>chemo</code> is only about 0.06, which is very small. So for example the slope of -0.1898 for <code>chemo</code> can be interpreted as the average increase (actually a decrease) in <code>drop</code> per unit change in <code>chemo</code> provided <code>base</code> is held constant.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As defined earlier in this section, the value of $R^2$ is the ratio of the variance of the fitted values and the variance of the observed values of <code>drop</code>. Now $R^2$ is almost 0.55, noticeably higher than its value of roughly 0.4 in the simple regression of <code>drop</code> on <code>base</code> alone.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt"></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is a fact of mathematics that $R^2$ will increase if you include more predictor variables (unless they are perfectly uncorrelated with $y$). It is tempting, therefore, to include as many predictors as possible. However, this is not always useful, as can be seen if we regress <code>drop</code> on <code>base</code>, <code>chemo</code>, and <code>height</code>. The value of $R^2$ does increase, but by a very small amount; the 95% confidence interval for the slope of <code>height</code> contains 0; in any case, since <code>height</code> is correlated with both <code>base</code> and <code>chemo</code>, it is hard to intrepret any of the individual slopes. Therefore it is not an overall gain to include <code>height</code> as a predictor.</p>
<p>The quantity "Adj. R-squared" is a version of $R^2$ that has carries a penalty for using too many predictor variables. We will not go into the mathematics of this statistic. Just note that its value is 0.499 for the regression on the two predictors <code>base</code> and <code>chemo</code>, but it drops to 0.473 when <code>height</code> is included as well. A drop in the adjusted $R^2$ is often used as an indication that some of the predictors should not be used.</p></div></div></div>