<div id="ipython-notebook">
            <a class="interact-button" href="http://data8.berkeley.edu/hub/interact?repo=textbook&path=notebooks/heights.csv&path=notebooks/sat2014.csv&path=notebooks/Regression.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\(','\)']],
      processEscapes: true
    }
  });
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-regression-line">The regression line<a class="anchor-link" href="#The-regression-line">¶</a></h2><p>The concepts of correlation and the "best" straight line through a scatter plot were developed in the late 1800's and early 1900's. The pioneers in the field were Sir Francis Galton, who was a cousin of Charles Darwin, and Galton's protégé Karl Pearson. Galton was interested in eugenics, and was a meticulous observer of the physical traits of parents and their offspring. Pearson, who had greater expertise than Galton in mathematics, helped turn those observations into the foundations of mathematical statistics.</p>
<p>The scatter plot below is of a famous dataset collected by Pearson and his colleagues in the early 1900's. It consists of the heights, in inches, of 1,078 pairs of fathers and sons.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="c"># Scatter plot using matplotlib method</span>
<span class="n">heights</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'heights.csv'</span><span class="p">)</span>
<span class="n">heights</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_3_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice the familiar football shape with a dense center and a few points on the perimeter. This shape results from two bell-shaped distributions that are correlated. The heights of both fathers and sons have a normal distribution.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">heights</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">55.5</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_5_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On average, the sons are about an inch taller than the fathers (perhaps due to improved nutrition).</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'father'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.99740259740261195</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The correlation between the heights of the fathers and sons is about 0.5.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">r</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">)</span>
<span class="n">r</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.50116268080759108</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-regression-effect">The regression effect<a class="anchor-link" href="#The-regression-effect">¶</a></h3><p>For fathers around 72 inches in height, we might expect their sons to be tall as well. The histogram below shows the height of all sons of 72-inch fathers.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">tall_fathers</span> <span class="o">=</span> <span class="n">heights</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">heights</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'father'</span><span class="p">))</span> <span class="o">==</span> <span class="mi">72</span><span class="p">)</span>
<span class="n">tall_fathers</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="s">'son'</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">55.5</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_11_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Most (68%) of the sons of these 72-inch fathers are <em>less than</em> 72 inches tall, even though sons are an inch taller than fathers on average!</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">tall_fathers</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">72</span><span class="p">)</span> <span class="o">/</span> <span class="n">tall_fathers</span><span class="o">.</span><span class="n">num_rows</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.6842105263157895</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In fact, the average height of a son of a 72-inch father is less than 71 inches. The sons of tall fathers are simply not as tall in this sample.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tall_fathers</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>70.728070175438603</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This fact was noticed by Galton, who had been hoping that exceptionally tall fathers would have sons who were just as exceptionally tall. However, the data were clear, and Galton realized that the tall fathers have sons who are not quite as exceptionally tall, on average. Frustrated, Galton called this phenomenon "regression to mediocrity."</p>
<p>Galton also noticed that exceptionally short fathers had sons who were somewhat taller relative to their generation, on average. In general, individuals who are away from average on one variable are expected to be not quite as far away from average on the other. This is called the <em>regression effect</em>.</p>
<p>The figure below shows the scatter plot of the data when both variables are measured in standard units. The red line shows equal standard units and has a slope of 1, but it does not match the angle of the cloud of points. The blue line, which does follow the angle of the cloud, is called the <em>regression line</em>, named for the "regression to mediocrity" it predicts.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">heights_standard</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_columns</span><span class="p">([</span>
        <span class="s">'father (standard units)'</span><span class="p">,</span> <span class="n">standard_units</span><span class="p">(</span><span class="n">heights</span><span class="p">[</span><span class="s">'father'</span><span class="p">]),</span>
        <span class="s">'son (standard units)'</span><span class="p">,</span> <span class="n">standard_units</span><span class="p">(</span><span class="n">heights</span><span class="p">[</span><span class="s">'son'</span><span class="p">])</span>
        <span class="p">])</span>
<span class="n">heights_standard</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plots</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_17_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>scatter</code> method of a <code>Table</code> draws this regression line for us when called with <code>fit_line=True</code>. This line passes through the result of multiplying father heights (in standard units) by $r$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">father_standard</span> <span class="o">=</span> <span class="n">heights_standard</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">heights_standard</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'father (standard units) * r'</span><span class="p">,</span> <span class="n">father_standard</span> <span class="o">*</span> <span class="n">r</span><span class="p">)</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_19_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another interpretation of this line is that it passes through average values for slices of the population of sons. To see this relationship, we can round the fathers each to the nearest unit, then average the heights of all sons associated with these rounded values. The green line is the regression line for these data, and it passes close to all of the yellow points, which are mean heights of sons (in standard units).</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">rounded</span> <span class="o">=</span> <span class="n">heights_standard</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'father (standard units)'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">father_standard</span><span class="p">))</span>
<span class="n">rounded</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'father (standard units)'</span><span class="p">,</span> <span class="n">rounded</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plots</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span> <span class="o">*</span> <span class="n">r</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">r</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_21_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Karl Pearson used the observation of the regression effect in the data above, as well as in other data provided by Galton, to develop the formal calculation of the correlation coefficient $r$. That is why $r$ is sometimes called <em>Pearson's correlation</em>.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-regression-line-in-original-units">The regression line in original units<a class="anchor-link" href="#The-regression-line-in-original-units">¶</a></h3><p>As we saw in the last section for football shaped scatter plots, when the variables $x$ and $y$ are measured in standard units, the best straight line for estimating $y$ based on $x$ has slope $r$ and passes through the origin. Thus the equation of the regression line can be written as:</p>
$$(\textit{y in standard units}) = r \times (\textit{x in standard units})$$<p>That is,
$$
\frac{\mbox{estimate of}~y ~-~\mbox{average of}~y}{\mbox{SD of}~y}
~=~ r \times 
\frac{\mbox{the given}~x ~-~\mbox{average of}~x}{\mbox{SD of}~x}
$$</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The equation can be converted into the original units of the data, either by rearranging this equation algebraically, or by labeling some important features of the line both in standard units and in the original units.</p>
<p><img alt="regline" src="../images/regline.png"/></p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Calculation-of-the-slope-and-intercept">Calculation of the slope and intercept<a class="anchor-link" href="#Calculation-of-the-slope-and-intercept">¶</a></h3><p>The regression line is also commonly expressed as a slope and an intercept, where an estimate for y is computed from an x value using the equation</p>
$$y = \textit{slope} \times x + \textit{intercept}$$<p>The calculations of the slope and intercept of the regression line can be derived from the equation above.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="k">def</span> <span class="nf">slope</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">correlation</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">intercept</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">slope</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="p">[</span><span class="n">slope</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">),</span> <span class="n">intercept</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">)]</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>[0.51400591254559247, 33.892800540661682]</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is worth noting that the intercept of approximately 33.89 inches is <em>not</em> intended as an estimate of the height of a son whose father is 0 inches tall. There is no such son and no such father. The intercept is merely a geometric or algebraic quantity that helps define the line. In general, it is not a good idea to <em>extrapolate</em>, that is, to make estimates outside the range of the available data. It is certainly not a good idea to extrapolate as far away as 0 is from the heights of the fathers in the study.</p>
<p>It is also worth noting that the slope <em>is not</em> <code>r</code>! Instead, it is <code>r</code> multiplied by the ratio of standard deviations.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">correlation</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.50116268080759108</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fitted-values">Fitted values<a class="anchor-link" href="#Fitted-values">¶</a></h2><p>We can also estimate every son in the data using the slope and intercept. The estimated values of $y$ are called the <em>fitted values</em>. They all lie on a straight line. To calculate them, take a son's height, multiply it by the slope of the regression line, and add the intercept. In other words, calculate the height of the regression line at the given value of $x$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">"""Return the height of the regression line at each x value."""</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">slope</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">intercept</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">fitted</span> <span class="o">=</span> <span class="n">heights</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'son (fitted)'</span><span class="p">,</span> <span class="n">fit</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">))</span>
<span class="n">fitted</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>father</th> <th>son</th> <th>son (fitted)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>65    </td> <td>59.8</td> <td>67.3032     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63.3  </td> <td>63.2</td> <td>66.4294     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65    </td> <td>63.3</td> <td>67.3032     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.8  </td> <td>62.8</td> <td>67.7144     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>61.1  </td> <td>64.3</td> <td>65.2986     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63    </td> <td>64.2</td> <td>66.2752     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.4  </td> <td>64.1</td> <td>67.5088     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>64.7  </td> <td>64  </td> <td>67.149      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>66.1  </td> <td>64.6</td> <td>67.8686     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>67    </td> <td>64  </td> <td>68.3312     </td>
        </tr>
    </tbody>
</table>
<p>... (1068 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Residuals">Residuals<a class="anchor-link" href="#Residuals">¶</a></h3></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The amount of error in each of these regression estimates is the difference between the son's height and its estimate. These errors are called <em>residuals</em>. Some residuals are positive. These correspond to points that are above the regression line – points for which the regression line under-estimates $y$. Negative residuals correspond to the line over-estimating values of $y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">res</span> <span class="o">=</span> <span class="n">fitted</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'residual'</span><span class="p">,</span> <span class="n">fitted</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">)</span> <span class="o">-</span> <span class="n">fitted</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son (fitted)'</span><span class="p">))</span>
<span class="n">res</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>father</th> <th>son</th> <th>son (fitted)</th> <th>residual</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>65    </td> <td>59.8</td> <td>67.3032     </td> <td>-7.50318 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63.3  </td> <td>63.2</td> <td>66.4294     </td> <td>-3.22937 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65    </td> <td>63.3</td> <td>67.3032     </td> <td>-4.00318 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.8  </td> <td>62.8</td> <td>67.7144     </td> <td>-4.91439 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>61.1  </td> <td>64.3</td> <td>65.2986     </td> <td>-0.998562</td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63    </td> <td>64.2</td> <td>66.2752     </td> <td>-2.07517 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.4  </td> <td>64.1</td> <td>67.5088     </td> <td>-3.40879 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>64.7  </td> <td>64  </td> <td>67.149      </td> <td>-3.14898 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>66.1  </td> <td>64.6</td> <td>67.8686     </td> <td>-3.26859 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>67    </td> <td>64  </td> <td>68.3312     </td> <td>-4.3312  </td>
        </tr>
    </tbody>
</table>
<p>... (1068 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As with deviations from average, the positive and negative residuals exactly cancel each other out. So the average (and sum) of the residuals is 0.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Residual-plots">Residual plots<a class="anchor-link" href="#Residual-plots">¶</a></h2></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Suppose you have carried out the regression of sons' heights on fathers' heights.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">heights</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_37_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is a good idea to then draw a residual plot, a scatter plot of the residuals versus the values of $x$. The residual plot of a good regression looks like the one below: a formless cloud with no pattern, centered around the horizontal axis. It shows that there is no discernible non-linear pattern in the original scatter plot.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="k">def</span> <span class="nf">residual_plot</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">"""Plot the residuals (errors) of regression."""</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">fit</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">table</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s">'residuals'</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
<span class="n">residual_plot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_39_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Residual plots can be useful for spotting non-linearity in the data, or other features that weaken the regression analysis. For example, consider the SAT data of the previous section, and suppose you try to estimate the <code>Combined</code> score based on <code>Participation Rate</code>.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">sat2014</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s">'sat2014.csv'</span><span class="p">)</span>
<span class="n">sat2014</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>State</th> <th>Participation Rate</th> <th>Critical Reading</th> <th>Math</th> <th>Writing</th> <th>Combined</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>North Dakota</td> <td>2.3               </td> <td>612             </td> <td>620 </td> <td>584    </td> <td>1816    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Illinois    </td> <td>4.6               </td> <td>599             </td> <td>616 </td> <td>587    </td> <td>1802    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Iowa        </td> <td>3.1               </td> <td>605             </td> <td>611 </td> <td>578    </td> <td>1794    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>South Dakota</td> <td>2.9               </td> <td>604             </td> <td>609 </td> <td>579    </td> <td>1792    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Minnesota   </td> <td>5.9               </td> <td>598             </td> <td>610 </td> <td>578    </td> <td>1786    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Michigan    </td> <td>3.8               </td> <td>593             </td> <td>610 </td> <td>581    </td> <td>1784    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Wisconsin   </td> <td>3.9               </td> <td>596             </td> <td>608 </td> <td>578    </td> <td>1782    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Missouri    </td> <td>4.2               </td> <td>595             </td> <td>597 </td> <td>579    </td> <td>1771    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Wyoming     </td> <td>3.3               </td> <td>590             </td> <td>599 </td> <td>573    </td> <td>1762    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Kansas      </td> <td>5.3               </td> <td>591             </td> <td>596 </td> <td>566    </td> <td>1753    </td>
        </tr>
    </tbody>
</table>
<p>... (41 rows omitted)</p></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">sat2014</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'Participation Rate'</span><span class="p">,</span> <span class="s">'Combined'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_42_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The relation between the variables is clearly non-linear, but you might be tempted to fit a straight line anyway, especially if you never looked at a scatter diagram of the data.</p>
<p>The points in the scatter plot start out above the regression line, then are consistently below the line, then above, then below. This pattern of non-linearity is more clearly visible in the residual plot.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">residual_plot</span><span class="p">(</span><span class="n">sat2014</span><span class="p">,</span> <span class="s">'Participation Rate'</span><span class="p">,</span> <span class="s">'Combined'</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plots</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Residual plot of a bad regression'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_44_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This residual plot is not a formless cloud; it shows a non-linear pattern, and is a signal that linear regression should not have been used for these data.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-rough-size-of-the-residuals">The rough size of the residuals<a class="anchor-link" href="#The-rough-size-of-the-residuals">¶</a></h3><p>Let us return to the heights of the fathers and sons, and investigate the estimates based on using the regression line and the flat line (in yellow) at the average height of the sons. As noted above, the rough size of the errors made using the flat line is the SD of $y$. The regression line appears to do a better job of estimating sons' heights than the flat line does. Thus, the rough size of the errors made using the regression line must be smaller that that using the flat line. In other words, the SD of the residuals must be smaller than the overall SD of $y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">heights</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">)</span>
<span class="n">average_son</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plots</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">60</span><span class="p">,</span> <span class="mi">75</span><span class="p">],</span> <span class="p">[</span><span class="n">average_son</span><span class="p">,</span> <span class="n">average_son</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gold'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_47_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here, once again, are the residuals in the estimation of sons' heights based on fathers' heights. Each residual is the difference between the height of a son and his estimated (or "fitted") height.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">res</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>father</th> <th>son</th> <th>son (fitted)</th> <th>residual</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>65    </td> <td>59.8</td> <td>67.3032     </td> <td>-7.50318 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63.3  </td> <td>63.2</td> <td>66.4294     </td> <td>-3.22937 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65    </td> <td>63.3</td> <td>67.3032     </td> <td>-4.00318 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.8  </td> <td>62.8</td> <td>67.7144     </td> <td>-4.91439 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>61.1  </td> <td>64.3</td> <td>65.2986     </td> <td>-0.998562</td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>63    </td> <td>64.2</td> <td>66.2752     </td> <td>-2.07517 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>65.4  </td> <td>64.1</td> <td>67.5088     </td> <td>-3.40879 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>64.7  </td> <td>64  </td> <td>67.149      </td> <td>-3.14898 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>66.1  </td> <td>64.6</td> <td>67.8686     </td> <td>-3.26859 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>67    </td> <td>64  </td> <td>68.3312     </td> <td>-4.3312  </td>
        </tr>
    </tbody>
</table>
<p>... (1068 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The average of the residuals is 0. All the negative errors exactly cancel out all the positive errors.</p>
<p>The standard deviation of the residuals is about 2.4 inches, while the overall SD of the sons' heights is about 2.8 inches. As expected, the SD of the residuals is smaller than the overall SD of $y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'residual'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>2.4358716091393409</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>2.8148875206879027</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By what factor? Another remarkable fact of mathematics is that no matter what the data look like, the SD of the residuals is $\sqrt{1-r^2}$ times the SD of $y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'residual'</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.86535308826267487</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.86535308826267476</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Average-and-SD-of-the-Residuals">Average and SD of the Residuals<a class="anchor-link" href="#Average-and-SD-of-the-Residuals">¶</a></h3><p><strong>Regardless of the shape of the scatter plot:</strong></p>
<p>average of the residuals = 0</p>
<p>SD of the residuals $~=~ \sqrt{1 - r^2} \cdot \mbox{SD of}~y$</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The residuals are equal to the values of $y$ minus the fitted values. Since the average of the residuals is 0, the average of the fitted values must be equal to the average of $y$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">heights</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="k">True</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="../notebooks-images/Regression_58_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The SD of the fitted values is smaller than the overall SD of $y$. The fitted values range from about 64 to about 73, whereas the values of $y$ range from about 58 to 77.</p>
<p>So if we take the ratio of the SD of the fitted values to the SD of $y$, we expect to get a number between 0 and 1. And indeed we do: a very special number between 0 and 1.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son (fitted)'</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s">'son'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.50116268080759108</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">correlation</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="s">'father'</span><span class="p">,</span> <span class="s">'son'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.50116268080759108</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is the final remarkable fact of mathematics in this section:</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Average-and-SD-of-the-Fitted-Values">Average and SD of the Fitted Values<a class="anchor-link" href="#Average-and-SD-of-the-Fitted-Values">¶</a></h3><p><strong>Regardless of the shape of the scatter plot:</strong></p>
<p>average of the fitted values = the average of $y$</p>
<p>SD of the fitted values $~=~ |r| \cdot$ SD of $y$</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice the absolute value of $r$ in the formula above. For the heights of fathers and sons, the correlation is positive and so there is no difference between using $r$ and using its absolute value. However, the result is true for variables that have negative correlation as well, provided we are careful to use the absolute value of $r$ instead of $r$.</p></div></div></div>